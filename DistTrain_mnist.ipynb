{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + horovod + ipyparallel MNIST example\n",
    "\n",
    "In this notebook we will use ipyparallel to deploy a Keras + Horovod distributed training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "# External imports\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to ipyparallel cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          13958933 interacti       sh sfarrell  R      15:27      8 nid00[047-052,124,153]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "squeue -u sfarrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker IDs: [0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# Cluster ID taken from job ID above\n",
    "job_id = 13958933\n",
    "cluster_id = 'cori_{}'.format(job_id)\n",
    "\n",
    "# Use default profile\n",
    "c = ipp.Client(timeout=60, cluster_id=cluster_id)\n",
    "print('Worker IDs:', c.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment on the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stderr:0] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[stderr:1] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[stderr:2] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[stderr:3] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[stderr:4] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[stderr:5] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[stderr:6] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[stderr:7] \n",
      "/global/cscratch1/sd/sfarrell/conda/isc-ihpc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import socket\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Horovod for MPI synchronization routines\n",
    "import horovod.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] MPI rank 0, local rank 0, host nid00047\n",
      "[stdout:1] MPI rank 6, local rank 0, host nid00124\n",
      "[stdout:2] MPI rank 2, local rank 0, host nid00049\n",
      "[stdout:3] MPI rank 7, local rank 0, host nid00153\n",
      "[stdout:4] MPI rank 3, local rank 0, host nid00050\n",
      "[stdout:5] MPI rank 4, local rank 0, host nid00051\n",
      "[stdout:6] MPI rank 1, local rank 0, host nid00048\n",
      "[stdout:7] MPI rank 5, local rank 0, host nid00052\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "# Initialize horovod\n",
    "hvd.init()\n",
    "print('MPI rank %i, local rank %i, host %s' %\n",
    "      (hvd.rank(), hvd.local_rank(), socket.gethostname()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Data config\n",
    "n_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Training config\n",
    "batch_size = 128\n",
    "n_epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data on each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[stdout:1] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[stdout:2] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[stdout:3] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[stdout:4] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[stdout:5] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[stdout:6] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "[stdout:7] \n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Scale pixels to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Adjust learning rate based on number of workers.\n",
    "opt = keras.optimizers.Adadelta(1.0 * hvd.size())\n",
    "\n",
    "# Add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if hvd.rank() == 0:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training\n",
    "\n",
    "Training with horovod + MPI allows for synchronous distributed batch updates.\n",
    "\n",
    "We need to register the model synchronization callback and restrict checkpoint writing to a single worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.2006 - acc: 0.9389 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0382 - acc: 0.9878 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0254 - acc: 0.9917 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0158 - acc: 0.9945 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0146 - acc: 0.9955 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0117 - acc: 0.9959 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "[stdout:1] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.1985 - acc: 0.9404 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0394 - acc: 0.9873 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0266 - acc: 0.9916 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0195 - acc: 0.9935 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0154 - acc: 0.9948 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0148 - acc: 0.9948 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0115 - acc: 0.9963 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0106 - acc: 0.9961 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "[stdout:2] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.1986 - acc: 0.9396 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0397 - acc: 0.9871 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0269 - acc: 0.9916 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0210 - acc: 0.9929 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0169 - acc: 0.9946 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0142 - acc: 0.9953 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "[stdout:3] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.1984 - acc: 0.9412 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0401 - acc: 0.9875 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0266 - acc: 0.9914 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0209 - acc: 0.9926 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0144 - acc: 0.9954 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0114 - acc: 0.9961 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "[stdout:4] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.1986 - acc: 0.9399 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0394 - acc: 0.9877 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0258 - acc: 0.9915 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0167 - acc: 0.9943 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0140 - acc: 0.9953 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0122 - acc: 0.9963 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "[stdout:5] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.2034 - acc: 0.9383 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0402 - acc: 0.9872 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0295 - acc: 0.9906 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0211 - acc: 0.9928 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0170 - acc: 0.9943 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0107 - acc: 0.9964 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "[stdout:6] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.1976 - acc: 0.9409 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0390 - acc: 0.9876 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0258 - acc: 0.9915 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0170 - acc: 0.9942 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0133 - acc: 0.9954 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0119 - acc: 0.9958 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "[stdout:7] \n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      " - 19s - loss: 0.1994 - acc: 0.9403 - val_loss: 0.0436 - val_acc: 0.9863\n",
      "Epoch 2/8\n",
      " - 17s - loss: 0.0396 - acc: 0.9878 - val_loss: 0.0286 - val_acc: 0.9918\n",
      "Epoch 3/8\n",
      " - 17s - loss: 0.0267 - acc: 0.9914 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 4/8\n",
      " - 17s - loss: 0.0196 - acc: 0.9934 - val_loss: 0.0332 - val_acc: 0.9917\n",
      "Epoch 5/8\n",
      " - 17s - loss: 0.0165 - acc: 0.9944 - val_loss: 0.0316 - val_acc: 0.9924\n",
      "Epoch 6/8\n",
      " - 17s - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0375 - val_acc: 0.9925\n",
      "Epoch 7/8\n",
      " - 18s - loss: 0.0116 - acc: 0.9959 - val_loss: 0.0418 - val_acc: 0.9913\n",
      "Epoch 8/8\n",
      " - 17s - loss: 0.0102 - acc: 0.9966 - val_loss: 0.0390 - val_acc: 0.9929\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "]\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "#if hvd.rank() == 0:\n",
    "#    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=n_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n",
      "[stdout:1] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n",
      "[stdout:2] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n",
      "[stdout:3] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n",
      "[stdout:4] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n",
      "[stdout:5] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n",
      "[stdout:6] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n",
      "[stdout:7] \n",
      "Test loss: 0.03895845667719013\n",
      "Test accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isc-ihpc",
   "language": "python",
   "name": "isc-ihpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
